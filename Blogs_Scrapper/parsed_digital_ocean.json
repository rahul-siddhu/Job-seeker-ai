[
    {
        "heading": "How startups scale on DigitalOcean Kubernetes: Best Practices Part VI - Security",
        "author": "Kunju Perath",
        "postedAt": "Posted:October 8, 2024•12 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/digitalocean-kubernetes-best-practices-security",
        "content": "This article is the final part of a 6-part series on DigitalOcean Kubernetes best practices.In Part 5,we focused on disaster recovery, where we highlighted recovering from disasters like hardware failures, data center outages, downtime caused by human error, and, as we’ll focus on in this part, security breaches.Let’s picture a scenario, you’re a startup that just had a bad actor infiltrate your Kubernetes cluster and delete your database instance containing thousands of records of customer data. Thankfully, you’re following the best practices outlined in Part 5 and you were able to successfully restore your customer data from backups. However, some of the customer data the hacker accessed was sensitive. So now you have to let all your customers know about the leak (which affects the perception of the business) and rotate all leaked credentials. That’s a lot of work! And if you’re not following robust security practices, then it can take a long time to recover from this breach. In this part, we’re going to review security best practices which can be preventative measures that make it harder for bad actors to infiltrate your cluster, and if a leak occurs, correct measures that allow your business to recover effectively.There are three security concepts we want to highlight in this post. There are plenty more worth looking into and adopting depending on your use case but these three are fundemental and fairly easy to adopt for a production kubernetes environment.Zero Trust securityZero Trust is the idea that we assume we cannot trust any of our services on the same network, and so all requests between services must be authenticated and authorized. For example, we shouldn’t assume the communication between an app and the database service is secure. Instead we should assume it has been compromised and that verification is always needed before starting communication. This way of thinking is more realistic in the world of security, because it’s not about if your business will be hacked but rather when it will be hacked.Least PrivilegeLeast Privilege is the idea of scoping down permissions for a user or service down to the bare minimum required. This is so that when a user or service is compromised the hacker’s exploit is limited to what behavior the user and service has. While setting up granular permissions feels like one extra step that takes away time from deploying your brand new app to prod, loose permissions is the perfect gift for a hacker to abuse. One form of Least Privilege that’s commonly seen is when you set up access tokens with expiration and limited scope. When working with other DigitalOcean compute resources, you might have tokens in place that for example might expire in 30 days and have read-only access. However, there’s many other ways to limit privileges and we’ll explore some of the most effective ways to do so in a Kubernetes cluster.Encryption at Rest and Encryption in TransitEncryption at Rest is the idea of ensuring your application data is actually encrypted and not stored in plain text. Encryption in Transit is the idea of ensuring your secrets are being pulled in a secure way into your application. DOKS clusters do Encryption at Rest by having ETCD secret encryption enabled to protect customer data in the cluster. However, you’ll likely have your application credentials stored outside your DOKS cluster as well and that’s when it’s worth considering secret management solutions that allow for safe storage and retrieval of your secrets as well as syncing those secrets within the cluster.Checklist: Set up Network PoliciesBy default, networking within a Kubernetes cluster is pretty open. Any service can reach any other service within the cluster. Now this is far from ideal when a hacker is able to hijack a more exposed service that normally doesn’t talk to the database service but is able to do so now because there were no network policies preventing it.With the built-in network policies offered by Kubernetes, you can control which pods are able to talk to which while still being able to communicate across namespaces. This is done at the IP level in that you can specify which network connections (and connection types) are allowed between pods.Cilium network policies support in DOKSWhile built-in policies are great when starting out, a more powerful option supported by DOKS is Cilium network policies provided by the open source Cilium CNI. These policies are more flexible because they associate pods with Cilium identities rather than IPs. Cilium identities are based on Kubernetes labels and so in a more dynamic environment where pods are restarted, the Cilium identity will continue to be associated with the pod even if the pod has a new IP. Being more efficient with the management of network policies among resources means less overhead when scaling. Furthermore, DOKS has built-in support for Cilium Hubble which provides a nice UI to view and monitor your network traffic in detail and get security insights.Checklist: Use mTLS to encrypt and authenticate trafficNetwork policies allow us to control where traffic is allowed to go in a cluster. But it doesn’t handle the encryption and authentication of that traffic between applications via a mechanism like TLS. One of the most effective ways to achieve this in a kubernetes cluster is to invest in a service mesh like Istio or Linkerd.Let’s say we have a service called Service A and a database service called Service D in our cluster. We set up network policies so that Service A is supposed to be the only one that’s able to talk to D. But how does Service A know that the service it’s reaching out to, Service D, is indeed the service fronting the database for this app. More importantly, how does Service D know the service that’s reaching out for information from the database is indeed Service A? It doesn’t if the traffic is unencrypted and unauthenticated.Man in the middleIn the situation described earlier with services A and D, it’s possible that a hacker is able to sit in the middle of the communication between these two services. At some point the hacker may try to spoof Service A and start making queries to Service D just like Service A could. If Service D has no way of verifying the service that’s requesting information from the database is indeed Service A then it will willingly give out information to the hacker.This is what mutual TLS (mTLS) solves and it is the mechanism which ensures the 2 mutual services, A and D, are indeed who they say they are. Both services A and D need to verify each other’s identities with TLS and establish a secure connection before starting communication. This embraces the concept of Zero Trust as we don’t assume if either service is who they say they are until we verify with TLS. So if Service D can’t verify the service that’s trying to talk to it is Service A, then it doesn’t establish a connection with this service, and so protecting access to the database. Quickly setting up mTLS for all your services in the cluster is one of the key benefits of introducing a service mesh.Benefits of using a service meshIt should be noted TLS encryption can be done without a service mesh but configuring it in every service, especially as the number of services in your organization grows, can become demanding and that’s where a service mesh makes things easier.Furthermore, one of the key benefits of a service mesh is network segmentation. This allows you to divide up networking within your cluster and control which service can talk to which. Now you might be thinking, isn’t that what network policies do? Yes, but network policies work at the IP layer while this is at the application layer. So you don’t need network policies for mapping service to service communication but they can still be useful to block traffic before reaching the workloads within the mesh.Checklist: Use RBAC to limit access controls with the Kubernetes APIAs mentioned prior, network policies and a service mesh are great ways to limit access for service-to-service communication. However, you might have services that need to interact dynamically with kubernetes resources (like secrets) and so you need to have it directly talk to the kubernetes API. Using KubernetesRBACthrough service accounts allows you to limit access granularly. By scoping permissions down to the bare minimum required by a service, you are adopting the idea of least privilege. So if a service is hijacked, then a hacker cannot make it do anything beyond what it is already capable of, and so limiting their ability to exploit.Checklist: Invest in a secret managerWhen managing application secrets outside your cluster, a good secret management solution uses the latest encryption standards to keep your data secure (Encryption at Rest), and furthermore, makes it so that retrieving the secrets is also secure (Encrypted in Transit).It should also have a good API that makes it easy to pull the latest secret values, so your application can be injected with the credentials on every new deployment. HashiCorp Vault has become the de facto open source standard for secret management.Quickly rotating secrets with a Kubernetes operatorGreat so you’re managing secrets in your secret manager. However, let’s say your organization’s credentials were backed up in a third party service, and that service announced they had a data breach. So now you have to rotate every leaked secret in your Kubernetes cluster. This could mean manually going into the secret manager UI, updating the leaked secrets, and redeploying the changes to your cluster. A quicker cloud native solution is to use a secrets operator. By secrets operator, I mean a Kubernetes operator that syncs your secret manager with the Kubernetes secrets in your cluster. So if you update the secret in the secret manager, the operator notices the change and reflects that change in the Kubernetes secret - no re-deploy required! The Vault Secrets Operator (VSO) is the open source solution offered that achieves this in Vault.Checklist: Secure your containers!One thing that’s easy to overlook is the most fundamental part of your Kubernetes cluster, which is the containers inside the pods hosting your applications. Containers by default in Kubernetes are still very open. And for most of your applications they don’t need such loose permissions (remember loose permissions is the best gift a hacker can get during their exploit). Here are some ways you can make your containers more secure:Run as non-rootMost of your applications likely don’t need to be running as the root user. Running as root allows the hacker to escape the container and access the host file system (“container breakout”). So unsetting root access is what you should do for most of your pods, however for the pods that do require elevated permissions to achieve certain behavior, this can still be done without root access.CapabilitiesLinux capabilities make it so you can enable selective elevated actions for the container without being a root user. For example, adding the capability to bind to privileged ports (like port 80) using the capability NET_BIND_SERVICE. On container startup, this capability will be enabled and so when the process tries to bind to port 80, the kernel will notice this is enabled and allow it to proceed even if the process is non-root.Seccomp ProfilesTo further adopt the idea of Least Privilege, seccomp (secure computing mode) profiles are an even more granular way to limit process behavior by limiting what system calls are allowed. This can be used separately or in conjunction with capabilities. For example, let’s say you want to allow a non-root process to bind to port 80, and so you use the capability NET_BIND_SERVICE but you also don’t want to allow the setsockopts system call (to prevent port reuse) so in addition you would also add a seccomp profile to block that system call. What’s convenient about seccomp is that you don’t have to create a custom profile and ensure you haven’t missed any powerful system calls. Instead there is a default profile called RuntimeDefault which exists to disable many powerful system calls not needed by containers. However, to be as strict as possible with your profiles it’s a good idea to start with the default and modify as needed to meet your use case. Furthermore, there is a way to notify system calls to see if there are any that should be blocked with theseccomp notifier. After identifying any additional system calls that you believe should be blocked, a slow rollout of the seccomp profile is a sound way to ensure your workloads are not disrupted.Here’s an example pod spec that applies capabilities and seccomp:apiVersion:v1kind:Podmetadata:name:more-secure-podspec:...securityContext:# set to non-zero to run as non-rootrunAsUser:1000# set capability that allows managing network settingscapabilities:add:-NET_ADMIN# drop all other capabilities so only NET_ADMIN is supporteddrop:-ALLseccompProfiles:# don't allow powerful system calls not needed by the containertype:RuntimeDefaultCopyDon’t use the latest tagIn a production environment, especially where application versioning is important, you shouldn’t use the latest image tag for your container image. While it is convenient during early development, if a bad actor infiltrates your container registry then it’s possible for them to push a bad image which will be pulled by your pod on your next deployment (or sooner depending on the image policy).Scan your imagesScanning your images is an automated way to ensure that there is no misconfiguration or vulnerabilities in the images loaded into your cluster. Usually you can run image scanning in your CICD pipeline, but some also provide a kubernetes operator that will scan your workloads in the cluster. An image scanning tool like Trivy can be used in both ways.Checklist: Automating security checks with webhooksSo now you’ve applied the best practices listed above to your workloads, but what about future workloads? Is every developer going to remember to do this? Probably not so one thing you could do is write documentation on the best practices for securing your workloads. But what if someone doesn’t see this document or forgets a step? And it’s likely you’ll have to keep this up to date. This is where automating this behavior with guardrails (or “policies” to comply with) is the better way to ensure everyone at your company is deploying secure workloads and adhering to general best practices in your cluster. Open Policy Agent (OPA) is an engine where you can define policies as code and it can be used generically with other technologies, not just Kubernetes. This means by itself it doesn’t tightly integrate with Kubernetes, but that’s where OPA Gatekeeper comes in.OPA GatekeeperThe OPA Gatekeeper project comes with Kubernetes integrations like CRDs, pre-defined policies and most importantly comes with mutating and validating webhooks which will ensure any defined policies are adhered to before any request is satisfied by the Kubernetes API server. This means any request to create a new resource will be modified (by the mutating webhook) and validated (by the validating webhook) before successfully being created. If the validation fails then it’s indicative to the developer that their workload is not complying with all the required policies. Learn more about OPA Gatekeeperhere.ConclusionOkay so you’ve set up network policies to restrict what types of connections and IP ranges are allowed in your cluster, a service mesh which ensures every service knows who they’re supposed to be talking to, and a secret manager for managing application credentials inside and outside the cluster effectively. Furthermore, you’ve updated your containers to follow best practices, and set up webhooks that will enforce policies outlined by your company for all future workloads. You’re on a roll!However, this blog post is not an exhaustive list of every possible security measure you could take with your workload, rather its goal is to highlight some of the most effective ways you can secure critical parts of your DOKS workloads. So where can you find more information about securing your workloads? Reference OWASP! The Open Web Application Security Project (OWASP) specifies best practices for securing your workloads in general, whether that’s on a kubernetes cluster or on a single machine like a droplet. For kubernetes specifically, there’s a great blog post by OWASP that mentionsThe Top 10 Security Risks in Kubernetes. Some of the risks we highlighted in here are mentioned there as well.Thank you!Thank you for taking part in this six part series on How SMBs and Startups Scale on DigitalOcean Kubernetes! We hope that this series has been an informative step in your Kubernetes journey!"
    },
    {
        "heading": "Introducing new GitHub Actions for App Platform",
        "author": "Markus Thömmes",
        "postedAt": "Posted:September 26, 2024•8 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/github-actions-for-app-platform",
        "content": "GitHub Actions is the CI/CD platform our customers use the most for building and deploying their code. In the past, we’ve provided a supported action, called app_action, that could be used to update an App that already exists via a GitHub Action.Today, we’re excited to introduce completely overhauled GitHub Actions for App Platform with improved pluggability to cater to all of the deployment needs you might think of.GitHub for all the thingsThe newdeployaction is the heart of our GitHub Actions ecosystem. Like the old one, it allows you to update an app that already exists. However, it also does much more than that: It also allows you to completely make the respective GitHub repository the source of truth in a GitOps-style fashion. Now, you can commit an AppSpec to your GitHub repository and handle the entire deployment process via GitHub Actions. It is no longer necessary to interact with DigitalOcean directly at all (apart from generating the token for the action to use).The in-repository AppSpec can also contain environment variable placeholders that will be replaced before deploying the new spec. This can be used to update image references on the fly or to manage your secrets via Github’s secret mechanism.We’ve also used the opportunity to provide more integration into the GitHub Actions ecosystem. The deploy action outputs the resulting app metadata and the build and deployment logs from the deployment that took place. Optionally, those logs are also logged in the action’s output itself. That metadata can be used to create rich integrations with App Platform, tailored to your specific needs.Deploy an app from GitHubTo deploy an app purely from GitHub without needing to create it out of band first is as simple as committing the respective App Spec to the repository (the action defaults to.do/app.yaml) and setup an action like below. This will cause the app to redeploy whenever a new commit is pushed to main (note thatdeploy_on_pushshould be turned off in the App Spec for that matter). Any changes to the App Spec itself would also be applied.name:Update Appon:push:branches:[main]permissions:contents:readjobs:deploy-app:runs-on:ubuntu-lateststeps:-name:Checkout repositoryuses:actions/checkout@v4-name:Deploy the appuses:digitalocean/app_action/deploy@v2with:token:${{secrets.DIGITALOCEAN_ACCESS_TOKEN}}CopyDeploy an app from an image built in the GitHub ActionAs a slightly more involved use-case, the below action builds an app from a Dockerfile in the repository inside the GitHub Action, not as part of the App Platform build. That image is then deployedby digestto ensure thatthis exactimage is what’s being deployed as part of the app.Note that the image digest is provided as theSAMPLE_DIGESTenvironment variable here. That’ll have to be referenced in the App Spec with the${SAMPLE_DIGEST}notation.name:Build,Push and Deploy a Docker Imageon:push:branches:[main]permissions:contents:readpackages:writejobs:build-push-deploy-image:runs-on:ubuntu-lateststeps:-name:Checkout repositoryuses:actions/checkout@v4-name:Log in to the Container registryuses:docker/login-action@v3.3.0with:registry:ghcr.iousername:${{github.actor}}password:${{secrets.GITHUB_TOKEN}}-name:Build and push Docker imageid:pushuses:docker/build-push-action@v6.5.0with:context:.push:truetags:ghcr.io/${{github.repository}}:latest-name:Deploy the appuses:digitalocean/app_action/deploy@v2env:SAMPLE_DIGEST:${{steps.push.outputs.digest}}with:token:${{secrets.DIGITALOCEAN_ACCESS_TOKEN}}CopyNote that the image digest is provided as theSAMPLE_DIGESTenvironment variable here. That’ll have to be referenced in the App Spec with the${SAMPLE_DIGEST}notation like so.name:sampleservices:-name:sampleimage:registry_type:GHCRregistry:YOUR_ORGrepository:YOUR_REPOdigest:${SAMPLE_DIGEST}CopyPull request previewsAnd lastly, pull request previews is a great example of the power of orchestratability. This feature allows you to deploy a new app for every pull request and surface the respective live URL [1] and the respective build [2] and deployment [3] logs to the pull request author, avoiding merging code that breaks the app in production.With the new deploy action, creating such an integration in your repository becomes trivial. It comes with a specialized “PR-preview-mode”, which generates a unique app name for each pull request, sanitizes the app spec of potentially conflicting resources (for example, it drops domains and alerts), and updates all potential GitHub references to point to the respective PR’s branch.Conversely, there’s also a new delete action, that allows you to delete apps again. Usually, this is done when a pull request is closed or merged to clean up resources.Equipped with that, you can imagine an action like the below, which will deploy an app per pull request and upon successful deployment, will post a comment to the pull request with a link to the app. On failure, it will post a link to the action’s logs and collapsible sections for build and deploy logs respectively for quick debugging. The second action will make sure that the respective app is deleted when the pull request closes or is merged.name:App Platform Previewon:pull_request:branches:[main]permissions:contents:readpull-requests:writejobs:test:name:previewruns-on:ubuntu-lateststeps:-name:Checkout repositoryuses:actions/checkout@v4-name:Deploy the appid:deployuses:digitalocean/app_action/deploy@v2with:deploy_pr_preview:\"true\"token:${{secrets.DIGITALOCEAN_ACCESS_TOKEN}}-uses:actions/github-script@v7env:BUILD_LOGS:${{steps.deploy.outputs.build_logs}}DEPLOY_LOGS:${{steps.deploy.outputs.deploy_logs}}with:script:|const { BUILD_LOGS, DEPLOY_LOGS } = process.env\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: `:rocket: :rocket: :rocket: The app was successfully deployed at ${{ fromJson(steps.deploy.outputs.app).live_url }}.`\n            })-uses:actions/github-script@v7if:failure()with:script:|github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: `The app failed to be deployed. Logs can be found [here](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}).## Logs<details><summary>Build logs</summary>\\`\\`\\`\n              ${BUILD_LOGS}\\`\\`\\`\n              </details><details><summary>Deploy logs</summary>\\`\\`\\`\n              ${DEPLOY_LOGS}\\`\\`\\`\n              </details>`})Copyname:Delete Previewon:pull_request:types:[closed]jobs:closed:runs-on:ubuntu-lateststeps:-name:delete preview appuses:digitalocean/app_action/delete@v2with:from_pr_preview:\"true\"ignore_not_found:\"true\"token:${{secrets.DIGITALOCEAN_ACCESS_TOKEN}}CopyThe sky’s the limit!You can find the code and documentation atthe app_action GitHub repository. We’re excited to see what kind of an integration you can come up with! Give the new actions a try and bring your app platform deployments to the next level."
    },
    {
        "heading": "How SMBs and startups scale on DigitalOcean Kubernetes: Best Practices Part V - Disaster Recovery",
        "author": "David Hwang",
        "postedAt": "Posted:August 14, 2024•7 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/how-isvs-startups-scale-digitalocean-kubernetes-best-practices-disaster-recovery",
        "content": "This article is part of a 6-part series on DigitalOcean Kubernetes best practices.InPart 4 of the series, we covered “scalability” best practices. We discussed knowing your DigitalOcean account limits, using the horizontal pod autoscaler and the buffer node, optimizing the application start-up time, scaling the DNS, scaling the caching and the database, validating the scalability of your application with network load testing, and adding resilience to Kubernetes API failures.In Part 5, we focus on disaster recovery. Developers working at small and medium-sized businesses (SMBs) tend to focus on building their applications. It’s easy for them to neglect backing up their cluster and data as they build. SMBs need to be prepared for unforeseen events, as disasters can be especially critical if they run their applications in a single region to optimize their cost, for example, or without any disaster recovery planning in place. We will look at what Disaster Recovery Planning (DRP) involves, explore common challenges, and lastly, provide a comprehensive checklist of best practices to help you prepare for disaster recovery.Disaster recovery planningProduction-ready cloud environments such as Kubernetes offer self-healing mechanisms for your workloads. Basic primitives such as Replicasets and Deployments, plus a good pod distribution strategy as described inpart 4 of this series, can prevent most disruptions for your applications’ end-users. Given enough resource capacity, it is safe to assume that a crashed workload will be restarted and pods on a crashed node will be redistributed to healthy nodes. Those cases are met on a day-to-day basis and are far from being disastrous.Unfortunately, we cannot solely rely on those Kubernetes mechanisms to save the day. Teams should be able to recover applications or the entire cluster in the event of an unexpected occurrence like a data center outage, hardware failures, downtime due to human error, or even security breaches.DRP involves planning the recovery of critical infrastructure pieces in the event of a disaster. A well-written and thoroughly tested procedure is the best way to resolve a critical incident with confidence and speed. Here are some of the requirements for a DRP:1. Understand the backup requirements and implement the recovery strategiesIdentify the namespaces/components of your cluster that are critical to business continuity and need to be included in the backup plan. It is important to find a cost-effective way to back up the cluster since it might not be necessary for you to back up all the Kubernetes objects, for example, if there is no persistent volume associated with them.SnapShooter, DigitalOcean’s backup and recovery solution, available on DigitalOcean Marketplace, allows you to back up the data on your cluster per namespace.Identify the Recovery Point Objective (RPO) and Recovery Time Objective (RTO) of the cluster. RPO defines the amount of data loss that your cluster can tolerate while RTO defines how long your cluster can remain unavailable. With SnapShooter, you can configure how often you want to backup your cluster and how long you want to retain your backup data depending on how those objectives are defined.2. Regularly test and verify backupsRegularly test and verify backups to ensure that they can be successfully restored. Automate this solution, or have a step-by-step document.3. Document the disaster recovery planDocument how to back up and restore the cluster with clear instructions that any team member can follow.Challenges of disaster recoveryDespite the ease of backing up and restoring cluster data with recovery solutions like SnapShooter, the disaster recovery process can remain challenging due to the distributed and dynamic nature of the Kubernetes infrastructure, which consists of multiple layers of services interconnected with each other. Disaster scenarios are often unique, with a differing set of problems and resolutions each time. Developers working at SMBs must mitigate downtime by preparing for common issues and considering the edge cases in advance.Here are some challenges with disaster recovery:1. Direct impact on business continuityMany applications critical to business run on top of the Kubernetes infrastructure and any failures can lead to an immediate disruption in their services. Data breaches or losses as well as extended downtime can lead to customers churning and potentially legal implications.2. Complexities in recovery due to edge casesDisaster recovery is sometimes not as simple as just deleting and restarting the application. It is necessary to preserve the load balancer and the associated DNS records. Otherwise, you will lose the external IP and need to recreate the DNS mapping which can take time to synchronize across users and lead to extended downtime. If you are using many certificates for various domains and an issuer like LetsEncrypt, you may encounter certificate rate limit issues while recovering your cluster. If you have secrets generated and stored internally in the cluster, they can be lost along with your cluster during the disaster. They will need to be regenerated manually, and can lead to extended downtime.3. Difficulty to reproduce, test, and validate the disaster scenariosThe root cause of a disaster may not always be obvious and it is not only difficult to simulate the disaster scenarios but also to automate the disaster recovery process.Kubernetes Best PracticesThis section describes a list of practices that could help your organization prevent or recover more quickly from disasters.Checklist: Use GitOps and secret managerInstead of manually applying the manifests, you can store the cluster’s state in a Git repository as a source of truth and reconcile the cluster state from there. In the GitOps model, a GitOps controller runs on the cluster and is responsible for synchronizing the state of the cluster with the specified Git location. In case of a critical failure, the latest manifests found in the trunk are applied, and the cluster is in its pre-failure state.Please keep in mind that secrets shouldn’t be exposed outside the cluster, particularly in a Git repository. This creates a challenge for the GitOps model where the manifests are kept in the Git repository. A sealed secrets controller to enable encryption of secrets outside the cluster is popular in the GitOps world but this is also vulnerable to disasters.We recommend using a secret manager to keep secrets out of the clusters. Examples include Vault or 1Password. To access the secrets from those secret managers, you just have to set up an external secrets operator on your cluster. This also helps ensure that the secrets are not lost during a disaster and can be easily fetched once the cluster is restored.Checklist: Keep state outside of the clustersWhen applications using persistent volumes (PV) crash or disk data gets corrupted from hardware failures and needs to be restored, they need the last working copy of the application volume/data. Otherwise, the lost data will be irreversible, and the newly created application will start in an empty state.There is a backup and restore solution with the Etcd and VolumeSnapshot offered by Kubernetes, but the process can be simplified with a backup and recovery solution likeSnapShooter.By keeping the state outside of the cluster, the Kubernetes resources are not lost along with the disasters. They can be easily restored to the cluster promptly and migrated to another cluster if necessary.Checklist: Schedule backupsSnapShooter allows you to configure the backup schedule, such as how frequently you want to back up your cluster data. You can also configure the retention policy to tell how long you want to retain the backup data. Scheduled backups help ensure that the backups are always up to date and will recover the latest data.Testing and validating your backup recovery procedures is important to confirm that the backup data is consistent and reliable. Restore backups in a new cluster and validate that the restored resources, including but not limited to the configmaps, secrets, and PVs, match the backup source.Checklist: Prefer high availability setupsDigitalOcean Kubernetes provides a high availability (HA) option that increases uptime and provides 99.95% SLA uptime for the control planes. The default control plane runs a single replica of each component and some downtime will occur during unexpected failures as components are restarted. If you enable high availability for a cluster, multiple replicas of each control plane component are created, helping to ensure that a redundant replica is available when a failure occurs.To enable HA while creating your cluster, select theAdd high availabilitycheckbox under theGet extra reliability for critical workloadssection. You can also update an existing cluster to enable high availability on the Control Panel or with a doctl command such asdoctl kubernetes cluster update example-cluster --auto-upgrade --maintenance-window saturday=02:00.Checklist: Place guardrailsConsider adding anadmission webhookto prevent misconfiguration of Kubernetes resources. Kubernetes allows you to define two types of admission webhooks— the validating admission webhook and the mutating admission webhook. You can refer to the implementation of theadmission webhook servervalidated in a Kubernetes e2e test to write your own webhook server. This webhook basically adds a layer of validation to help protect your cluster from unintended consequences.You can also consider granting permissions to each user throughRole-based Access Controls (RBAC)or manage permissions with service accounts. Both of these techniques involve defining a cluster role with relevant permissions to different resources and binding the user or the service account with a cluster role binding. DigitalOcean is adding support for the modifier and the resource viewer role along with other standard roles in 2024, which you can configure for each team member on the Cloud Control Panel. Instead of giving everyone full access, RBAC will only allow relevant people to modify or delete resources.Next stepsIn the final part of our Kubernetes adoption journey series, we will delve into securing your Kubernetes environment, covering best practices for network policies, access controls, and securing application workloads. Enhancing your infrastructure’s security is the last crucial part of navigating the complexities of Kubernetes. Stay tuned for insights to help empower your Kubernetes journey.Ready to embark on a transformative journey and get the most from Kubernetes on DigitalOcean? Sign up forDigitalOcean Kubernetes here."
    },
    {
        "heading": "How to Migrate Production Code to a Monorepo",
        "author": "Brian Holt",
        "postedAt": "Posted:August 13, 2024•11 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/migrate-production-code-to-monorepo",
        "content": "In February 2024, the UI Platform team moved 1.3M lines of React micro-frontend code to a monorepo while retaining git history. Our team is responsible for the frontend architecture and UI Engineer experience at DigitalOcean, and moving to a monorepo is part of our frontend vision, of which much is lifted fromMonica Lent’s Building Resilient Frontend Architecture talk. With a monorepo, we aimed to reduce our dependency management burdens and simplify our micro-frontend boilerplate to ultimately increase developer velocity.While there are plenty of guides for getting started with monorepos, there are few that touch on migrating existing repositories over. This is the guide I wish I had when we started and I hope it helps someone else!What is a monorepo?A monorepo is a collection of isolated packages that live in a single repository. It reduces friction between shared code while keeping the safety gained from isolation. In contrast to a monolithic repository where the entire application is deployed as one, a monorepo allows packages to be deployed on their own.Approach: moving to a monorepoWe’re fans of Kent Beck’s famous refactoring quote, “First make the change easy (warning this may be hard), then make the easy change”, and applied it to this work as best we could. In its essence, a monorepo is code colocation, so we restricted the actual migration to that alone; there would be no functional change in any of the apps but they would live next to each other. Any changes required to an app would get applied while it was in its own repo, so problems with colocation were isolated.Our apps had been created over a period of roughly three years, and in many cases, the things that were learned from newer apps were not applied to older apps. It created a fair bit of inconsistency which added complexity to colocation and kicked off refactoring cycles. As we worked through each app, they needed to: run the local dev environment, tests, linters, and IDE plugins; run the CI/CD pipelines; and deploy to our staging environment. At least one of those steps broke with any two apps colocated, so we’d refactor the independent repos until the problem was resolved. Eventually,anytwo apps worked together, which actually meant all of the apps worked together.For this article, I’ll break the project into three stages, though some pre-migration steps only became apparent as we worked through the task:Pre-migration: making the change easyMigration: colocating the appsPost-migration: optimizing the monorepoPre-migration: making the change easyScriptingWe made automation our guiding principle–every change needed to be run from a script so that it was reproducible from scratch. We usedzxso we could use both Node and CLI tooling in the same script. As we solved problems through refactoring, we’d update the script and template files (that mimicked the file structure of the future monorepo) and re-run it. We ran the script hundreds of times as it evolved and were able to eliminate human error on the day of the final migration because of the approach.The script ran from an external repo so it wouldn’t be overwritten by force pushes, and performed the following steps:Initialized git in a temporary monorepo.Cloned each repo into a temporary folder.Removed things that would become irrelevant after migration and couldn’t be completed prior, like deleting yarn.lock and .nvmrc.Created a move commit that put all the files in the correct workspace folder.Merged the unrelated histories from local remotes.Copied the template files into the monorepo.and finally force-pushed the repository.This is it, with annotations, in its entirety:process.env.FORCE_COLOR='1';import{$,path,os,cd,spinner}from'zx';constSCRIPT_ROOT=path.resolve(__dirname);constMONOREPO=path.join(os.tmpdir(),`monorepo-${Date.now()}`);constREPO_PREFIX='git@github.com:username/';constREPO_SUFFIX='.git';// repo names to fetch from GithubconstREPOS=['repo-a','repo-b'];// 1. Initialize git in monorepocd(MONOREPO);await$`mkdir -p${MONOREPO}/apps/`;await$`git init`;await$`git commit --allow-empty -m \"Initial commit\"`;cd(SCRIPT_ROOT);// Merge git histories loopforawait(constrepoofREPOS){constrepoUrl=`${REPO_PREFIX}${repo}${REPO_SUFFIX}`;consttempRepo=path.join(os.tmpdir(),`${repo}-${Date.now()}`);// 2. Clone the app into a temporary folderawait$`mkdir -p${tempRepo}`;await$`git clone${repoUrl}${tempRepo}`;cd(tempRepo);// 3. Remove these files and folders because they're no longer necessary and it speeds up this scriptawait$`rm -f .gitignore .gitattributes .github .nvmrc yarn.lock node_modules .yarn build`;// try…catch so non-zero exit codes don't stop the script from continuingtry{await$`git add .`;await$`git diff --staged --quiet || git commit -m \"[${repo}]: Remove conflicting files\" --no-verify`;}catch{}// 4. Create a move commit// In order to preserve git history accurately, we need to create a// move commit from the root of the sub-repo into a directory that// imitates the monorepo ie. from ./ to ./apps/constmainBranch=(await$`git branch --show-current`).stdout.trim();await$`mkdir -p apps/${repo}`;await$`git ls-tree${mainBranch}--name-only | xargs -I{} git mv {} apps/${repo}`;await$`git commit -m \"[${repo}]: Move${repo}to app/${repo}\"`;cd(MONOREPO);// 5. Merge git history using local remote so changes wouldn't break live codebasesawait$`git remote add${repo}${tempRepo}`;await$`git fetch${repo}`;await$`git merge --allow-unrelated-histories${repo}/main`;await$`git remote rm${repo}`;}// 6. Copy template filescd(SCRIPT_ROOT);await$`cp -a monorepo-template/.${MONOREPO}`;cd(MONOREPO);// Create fresh yarn.lock, yarn install exits with non-zerotry{await$`yarn install --refresh-lockfile`;}catch{}await$`git add .`;await$`git commit -m \"Init monorepo\"`;// 7. Rebuild the monorepo every timeawait$`git remote add origin git@github.com:username/your-new-monorepo.git`;awaitspinner(()=>$`git push -f origin main`);console.log('🎉 monorepo is live');CopyGithub Action workflowsUpdating our CI/CD jobs in Github Actions to support running both single- and multi-app repositories was one of the first tasks. We passed aworking-directoryinto shared actions so each job would run from the application’s folder instead of the root as if it were in a single-app repository. We usedworking-directoryas the input parameter name and set the default to’.’for backwards compatibility.Our deploy workflows had custom keys, likeapp_nameandservice_id, which were hard-coded strings in each repo’s deploy workflow. We extracted these values into another file and added a step to read them so our workflow actions could be generic.In the templated files, we built an action that could detect what workspaces changed, then would return amatrixto fire off subsequent jobs for only changed workspaces. It reduced wasted Github Action time, but also prevented more critical things like unnecessary deployments or e2e jobs from running.Yarn 4 upgradeAfter a couple of days attempting to fix inter-app dependency conflicts in Yarn 1, we decided upgrading toYarn 4was a required milestone because of its improvedworkspacessupport. WithnmHoistingLimitsset toworkspaces, each app could contain conflicting dependencies, effectively running in isolation.Yarn has agreat migration guideand was painless for the most part. We broke the work into two pull requests per application: explicitly add undeclared dependencies as perYarn’s rules; and complete the upgrade to Yarn 4. In practice, I upgraded each app locally, then ranyarn dlx @yarnpkg/doctorandnpx depcheckto identify the missing packages. Once I had the list, I reinstalled them on a new branch to safely separate changed dependencies from the Yarn upgrade.The way Yarn is installed has fundamentally changed between version 1 and 4, so I needed to support the team when they ran into issues upgrading on their machines. In all cases, the problems stemmed from location issues, typically with the wrong version of Yarn running. Node,Corepack, and Yarnallneed to be installed within your Node version manager, like/Users/you/.nvm/versions/node/v20.9.0/bin/node. You can check the locations with:which node\n# should output something like /Users/you/.nvm/versions/node/v20.9.0/bin/node\n\n# if you're using nvm and you get something else run:\n\n# nvm use\n\n\n\nwhich corepack\n\n# should output something like /Users/you/.nvm/versions/node/v20.9.0/bin/corepack\n\n# if you get something else run:\n\n# corepack enable\n\n\n\nwhich yarn\n\n# should output something like /Users/you/.nvm/versions/node/v20.9.0/bin/yarn\n\n# if you get something else run:\n\n# corepack installMigration: colocating the appsOnce all apps were running as expected, we announced a migration date and the full plan. LikeStripe’s migration from Flow to TypeScript, we wanted developers to leave Friday afternoon and start work Monday morning in the brand new codebase with no ceremony.On the day of, we posted steps in Slack so there was a clear record in case anything went wrong and that anyone watching could follow along. The steps were largely double-checks, but obviously included the actual migration too.We ran through one last review of the build script and template files then compared it against the last working run.We ran the script for the last time, rewriting the repo history again with a force-push.We manually kicked off the PR CI/CD pipeline to confirm all the apps pass.We manually ran the staging deploy jobs to ensure all the apps deployed.We turned on branch protection, merge checks, permissions, and other repo settings, as well as enabled our automatic CI/CD jobs.And finally, we archived the old app repositories.We left instructions for getting started and held office hours for any engineers to drop in and troubleshoot each day for the following week. We also migrated a handful of open PRs that weren’t merged by the migration date with a couple commands from the command line:# From the archived repo, rebase your PR commits into a single commit, change the sha prefix to fixup\n\ngit rebase main -i\n\n\n\n# Run the move commit so all files live within an ./apps/ directory like the monorepo\n\n# This only moves changed files to reduce conflicts + commit noise\n\nAPP_NAME=REPLACE_THIS_WITH_YOUR_APP_NAME\n\nfor file in $(git diff main --name-only --cached); do target_path=$(dirname $file); mkdir -p \"apps/$APP_NAME/$target_path\"; git mv $file \"apps/$APP_NAME/$target_path\" -v; done;\n\n\n\n# Squash the commit to previous batch of PR commits\n\ngit commit --amend --no-edit\n\n\n\n# Copy the sha output\n\nSHA=$(git rev-parse --short HEAD)\n\n\n\n# In the monorepo\n\ncd monorepo\n\n\n\n# Checkout a new branch that matches the original PR name\n\ngit checkout -b …\n\n\n\n# Assuming the monorepo and original repo are in sibling folders, run\n\ngit --git-dir=../${APP_NAME}/.git format-patch -k -1 --stdout ${SHA} | git am -3 -k\n\n# Then open the new PRPost-migration: optimizing the monorepoThe following few weeks after the migration were spent tidying and optimizing it.We installeddependency-cruiserto restrict the ability to reach into sibling modules through the file system and instead require standard package importing. This keeps our monorepo code isolated and prevents a ball-of-mud from forming. The rule that enforces that looks like:{name:'apps-not-to-apps',comment:'One app should not reach into another app (in a separate folder)',severity:'error',from:{path:'(^apps/)([^/]+)/'},to:{path:'^$1',pathNot:'$1$2'},}CopyWe moved packages and settings (like Prettier and Browserlist) that were duplicated in workspaces into the root directory, and then standardized them. We also abstracted developer dependencies (like eslint, stylelint, Cypress, and Jest) into isolated workspaces under./packages, then imported them into each app withworkspace:*. These new packages are self-contained so all of their plugins and settings could be accessed with a single import, and so it would be easy to keep track of their versions.Our team made several Github Action improvements as scaling problems immediately surfaced when our pipelines ran across multiple applications.We changed allyarn installcommands toyarn workspaces focusso each workflow only installed the dependencies of an isolated app. The firstyarn installin a monorepo can take a long time, and we regularly hit workflow timeouts. It’s likely that switching toYarn Plug’n’Playwill speed up installs with a cache as well, but we’re not quite there yet.We gated our PR workflow jobs to reduce the time any workflow would complete and reduce the burden on parallel jobs. In order, the gates run: our build matrix job to determine what apps have changed; build commands and a non-matrix lint job that runs across the repo to reduce container setup time; unit-tests; and finally e2e tests. The tradeoff in parallelization has been well worth the reliability of successful runs, and we’ve moved individual jobs into combined workflows to reduce container setup time which has kept our total job time relatively unchanged.We addedrun-nameto our deploy workflows so it’s really easy to see what job is associated with which app. We also make heavy use of$GITHUB_STEP_SUMMARYfor both debugging jobs and reporting.We addedmax-parallelbecause some of our actions were getting rate-limited by external services.Finally, we added a.git-blame-ignore-revs filewith the shas of batch commits so they would get hidden from git history.ConclusionThis move took us one quarter to complete and was the largest frontend code migration at DigitalOcean thus far. We’ve seen the average number of React-relatedfeature PRs increase by 1.6x, and the average number of internallibrary bumps decrease by 95%. While it’s harder to get an accurate measurement, each batch of our library bumps used to take most of the day and can now be released and upgraded in under an hour. Soon we will completely eliminate those bumps withModule Federation. It’s also been significantly easier and safer to do sweeping changes, like fixing all our eslint errors and warnings, or upgrading third-party libraries.There’s always room for improvement and the two challenges we ran into were from the Yarn 4 upgrade and our CI/CD deploy pipeline. We hadn’t communicated how critical Yarn 4 was to the project and that it was our new norm for frontends, so we inadvertently left some team members behind with Yarn 1. When the monorepo launched, they were unable to get the repo running and we spent most of the first few days troubleshooting environments. Additionally, while we ranstagingdeploys both before and on the day of the migration, we failed to consider runningproductiondeploys which were slightly different. Our automated production pipeline was broken first thing Monday morning, but we luckily had it up again before lunch. For the next project, we’ve created a more robust release template that includes communication and support around required developer changes as well as better steps for the entire production process.Breaking work down as if it were a refactor worked extremely well. We were able to keep track of progress (even as new tasks were added) and point to discrete batches of work for both issues and successes. The approach felt measured and straightforward with very little room for surprises or risk. There are still things to optimize across our frontend architecture and the monorepo is helping us move through it much faster. If you’re starting with a brand new repo, I’d like to recommend these four articles that helped us along the way:Pixel Matters: How to manage multiple Front-End projects with a monorepoBasedash: Our TypeScript monorepo setupThe Ultimate Guide to TypeScript MonoreposEarthly: How to Set Up a TypeScript Monorepo"
    },
    {
        "heading": "Simplifying Distributed App Complexity with App Platform’s Log Forwarding and OpenSearch",
        "author": "Elan Hasson",
        "postedAt": "Posted:August 8, 2024•3 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/simplifying-distributed-app-complexity-with-app-platforms-log-forwarding-and-opensearch",
        "content": "As applications become more complex both in functionality and deployment footprint, observability becomes increasingly crucial. App Platform supportsmetricsvia our Insights features andloggingvia the Log Forwarding feature. In this post, we will build on the microservices architecture example by demonstrating how to enhance it with log forwarding toManaged OpenSearch.The Complexity of Distributed SystemsDistributed applications often lead to distributed problems that are difficult to debug. Microservices architecture, while powerful, introduces a layer of complexity that can be daunting. Services need to communicate effectively, manage state, and scale independently. All these factors contribute to an intricate web of interdependencies that can make troubleshooting a nightmare.Enter App PlatformApp Platform simplifies the deployment and management of distributed apps. It allows developers to focus on coding and application logic, while App Platform handles the infrastructure. With features likeLog Forwarding,Insights, andAlerts, it offers a streamlined approach to handling complex architectures.Building on the Microservices ArchitectureIn a previous post,Deploying your Microservices Architecture App in App Platform, we discussed how to deploy a microservices app with amanaged Kafkacluster and two components:Web Service API: Receives requests, publishes messages to Kafka, and responds to the user.Backend Processing Service: Consumes messages from Kafka and processes them.This architecture allows for independentscalingof the web service API and backend processing service, providing flexibility and efficiency.Enhancing Observability with Log Forwarding to OpenSearch (or ElasticSearch)Now, let’s enhance this architecture with improved observability using App Platform’s Log Forwarding feature. Logs are vital for understanding what is happening within your application. By forwarding logs to OpenSearch, you can gain deeper insights and make debugging easier.App Platform supports OpenSearch in two different ways:Bring-Your-Own: In this mode, you can use any OpenSearch or ElasticSearch endpoint as long as it supports TLS and authentication via a username and password.DigitalOcean Managed OpenSearch: You can create a cluster in a few minutes via the Cloud Console’s Databases page.In this example, we’ll use a managed OpenSearch cluster for simplicity.Step 1: Deploy the AppDeploy your app following the steps at describedin this postStep 2: Configuring Log ForwardingEnable Log Forwarding: In the App Platform dashboard, navigate to your app and enable log forwarding from the settings tab.Configure OpenSearch: Set up an OpenSearch instance where your logs will be sent. Ensure your OpenSearch endpoint is accessible and properly secured.Alternatively, you can modify the App Spec located in theKafka post repository.Here’s what the changes to the app spec would look likeyaml\n\n name: sample-golang-kafka-2\n\n# env and databases sections removed for brevity\n\nservices:\n\n  - name: producer\n\n    source_dir: producer\n\n    github:\n\n      repo: digitalocean/sample-golang-kafka\n\n      branch: main\n\n    log_destinations:\n\n      - name: logs-producer\n\n      open_search:\n\n        cluster_name: my-opensearch-cluster\n\nworkers:\n\n  - name: consumer\n\n    source_dir: consumer\n\n    github:\n\n      repo: digitalocean/sample-golang-kafka\n\n      branch: main\n\n    log_destinations:\n\n      - name: logs-consumer\n\n      open_search:\n\n        cluster_name: my-opensearch-clusterStep 3: Configure OpenSearch DashboardLogin via the link provided in the OpenSearch database dashboard using the credentials providedCreate anIndex Patternby navigating toManagement→Dashboards Management→Index PatternsUse the Default datasourceSpecify a pattern name of ‘logs*’Specify@timestampas theTime fieldand click theCreate index patternbuttonNavigate toOpenSearch Dashboards→DiscoverConclusionApp Platform is designed to simplify the complexities of deploying and managing distributed, cloud-native apps. By leveraging features like Log Forwarding and Managed OpenSearch, you can enhance observability and streamline your operational experience. Try out these features and let us know how they help you manage your distributed applications more effectively."
    },
    {
        "heading": "Enhancing Search Capabilities with K-NN Vector Search in OpenSearch",
        "author": "Dustin Wilson",
        "postedAt": "Posted:July 31, 2024•5 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/enhancing-search-capabilities-with-k-nn-vector-search-in-opensearch",
        "content": "Many applications depend on the ability to deliver precise and relevant search results. Although the full-text search capabilities of traditional relational databases are sufficient in some situations, these databases can fall short in extracting semantic meaning from text or searching through less-structured data. In this blog post, we’ll explore how you can address these limitations using DigitalOcean-managed OpenSearch and a collection of techniques called K-Nearest Neighbor vector search (K-NN). K-NN makes OpenSearch a powerful and flexible solution for various search and analytics applications.Understanding K-NN Vector SearchWhat is K-NN Vector Search?Unlike traditional search methods that rely on keyword matching,K-NN vector searchinvolves representing each record in a dataset as avectorthat encapsulates the attributes of the record. Machine learning models are often used toembeddata into a vector representation. When a query is made, the search engine computes the distance between the query vector and the data vectors and returns the nearest neighbors based on a predefined distance metric, such as Euclidean distance or cosine similarity.Why Use OpenSearch for K-NN Vector Search?Introduction to OpenSearchOpenSearchis a highly scalable open-source search and analytics engine. It builds upon the strengths of Elasticsearch, providing robust features for full-text search, log analytics, and more. With the introduction of vector search capabilities, OpenSearch extends its utility to more advanced use cases such as natural language processing, recommendation systems, and image retrieval.Benefits of Using OpenSearch for Vector SearchScalability:OpenSearch can handle large volumes of data and queries efficiently. Using approximate nearest neighbor algorithms, OpenSearch can provide relevant search results much faster and with a lower memory footprint.Flexibility:It supports various types of data and search functionalities, making it suitable for diverse applications.Community and Support:Being open-source, it benefits from a vibrant community and regular updates.Setting Up OpenSearch for K-NN Vector SearchInstalling OpenSearchTo get started, you need to install OpenSearch. Here’s a basic command to pull and run the latest version of the OpenSearch Docker image:docker pull opensearchproject/opensearch:latest\n\n\ndocker run -d --name opensearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"OPENSEARCH_INITIAL_ADMIN_PASSWORD=<your-strong-password>” opensearchproject/opensearch:latestNote:You need to set an initial admin password when you try to run the opensearch docker container.\nIt should be a minimum of 8 characters and must contain at least one uppercase letter, one lowercase letter, one digit, and one special character that is strong.Alternatively, DigitalOcean supportsManaged OpenSearch, which makes configuring and managing OpenSearch clusters a breeze.Configuring OpenSearch for Vector SearchAfter installing OpenSearch, the next step is to enable the K-NN plugin. On self-managed clusters, this involves modifying the cluster’s configuration file. On DigitalOcean Managed Opensearch The K-NN plugin is enabled by default and no additional configuration is required.Implementing K-NN Vector SearchTo use K-NN vector search, you must first create an index with vector fields. You can do so by navigating to the Opensearch development console athttps://${CLUSTER_HOST}/app/dev_tools#/consoleand submitting the following request. Alternatively, you can send these commands as HTTP requests tohttps://${CLUSTER_HOST}:9200.PUT /my_vector_index\n\n{\n\n  \"mappings\": {\n\n    \"properties\": {\n\n      \"my_vector\": {\n\n        \"type\": \"K-NN_vector\",\n\n        \"dimension\": 128\n\n      }\n\n    }\n\n  }\n\n}With this request you’ve created an index,my_vector_index, which you can use to store and query data using 128-dimension embeddings. You can now begin adding documents along with their vector representations to the index with the following request.PUT /my_vector_index/_doc/1\n\n{\n\n  \"my_vector\": [0.1, 0.2, ... , 0.128],\n\n  \"description\": \"Sample document\"\n\n}Finally, to perform a K-NN search over these documents, you can use the following query.POST /my_vector_index/_search\n\n{\n\n  \"size\": 5,\n\n  \"query\": {\n\n    \"K-NN\": {\n\n      \"my_vector\": {\n\n        \"vector\": [0.1, 0.2, ... , 0.128],\n\n        \"k\": 5\n\n      }\n\n    }\n\n  }\n\n}Use Cases and ApplicationsLet’s cover a few end-to-end applications that could make use of Opensearch’s K-NN capabilities.Customer Support Chatbot:Vector search is often used to find semantically similar texts. A chatbot service might use a machine-learning model to embed an incoming query (e.g. “How can I reset my password?”) into a vector and then use K-NN vector search to find similar queries in the knowledge base, such as “I forgot my password, how do I reset it?”. The chatbot can use this information to provide the user a more helpful response based on these similar queries.E-commerce Platform:K-NN vector search can enhance recommendation systems by finding items similar to a user’s preferences based on vector representations. For example, a user who buys a book from an online store might be recommended other books by the same author, books from the same genre, or even books that other users with similar preferences have bought. In this example, the vector representation of a book may include attributes like author, genre, ratings, and keywords from reviews.Fashion Retailer:By converting images into vectors using deep learning models, K-NN vector search can be used to retrieve visually similar images from a database. A user may upload a photo of a red dress. The system processes the image to create a vector representing the dress’s visual features. Using K-NN vector search, the platform retrieves and displays similar dresses in various shades of red, with similar cuts and designs, helping the user find exactly what they’re looking for.Challenges and Considerations using K-NN with OpenSearch1. Vector DimensionalityHigh-dimensional vectors can lead to increased computational complexity. It’s important to balance vector dimensions with performance requirements. Luckily, OpenSearch has multipleK-NN methodswith their own performance characteristics. While each method aims to return vectors with the minimal distance to an incoming vector, some can be tuned to prioritize memory use, response time or accuracy.2. Data NormalizationEnsuring that data is normalized and consistent is crucial for the accuracy of K-NN search results.3. Performance TuningOptimizing OpenSearch settings and hardware resources is essential for handling large-scale vector searches efficiently. See thisarticle for more details on performance tuning.ConclusionK-NN vector search opens up new possibilities for delivering highly relevant search results across various domains. By leveraging OpenSearch’s powerful capabilities, developers can implement advanced search functionalities with relative ease. Whether it’s for recommendation systems, image retrieval, or NLP applications, K-NN vector search with OpenSearch is a valuable tool in the search technology landscape."
    },
    {
        "heading": "Deploying your Microservices Architecture App in App Platform using Managed Kafka",
        "author": "Mavis Franco",
        "postedAt": "Posted:July 2, 2024•3 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/deploy-microservices-architecture-app-in-app-platform",
        "content": "Microservices ImplementationMicroservices architecture apps are characterized by organizing application components in such a way that they can be developed, tested, deployed and scaled separately. App Platform aims to make this model seamless by allowing the user to addmultiple components on the same app.A simple microservices architecture app may have a two components:Web service APIthat receives a request, publishes a message to a distributed event processing platform such as Kafka, and returns a response to the user immediately without waiting for the processing to finish. The user can later poll for a response status. This pattern provides a better user experience in terms of responsiveness to user requests. This component can be deployed in App Platform as aservice component type.Backend processing servicethat listens to events from a distributed event processing platform. This component can be deployed as aworker component typethat consumes the message and does the business logic needed to fulfill the user request.This type of microservices architecture allows developers to scale theweb service API(producer) and thebackend processing service(consumer) separately to meet application demands. For example, the backend service processing may be CPU resource intensive and benefit from aDedicated CPUinstance that can autoscale based on demand. On the other hand, the web service API may perform well with aShared CPUinstance with a fixed scale of 2 for redundancy.Managed Kafka IntegrationApp Platform support for apps with multiple components enables the producer and consumer pattern that microservices architectures need. However, we take it a step further to simplify the developer journey by implementing a seamless integration withDigitalOcean Managed Kafka Offering. More details on the integration can be found inhere.App Platform’s Kafka integration allows you to attach Kafka instances to your app and simplifies the environment variable configuration needed to connect to Kafka. This allows you to focus on building your app and not worry about the infrastructure setup.Our goal is to make this asDO Simpleas possible, here are the steps. Try it and let us know what you think!Step 1 - Configuring a Managed Kafka instanceInstructions on how to deploy a managed Kafka can be foundhere. Atopicis also required in order to publish and consume messages.Step 2 - Deploy Your AppDeploy your app with 3 components. Here is a sample reposample-golang-kafka repositoryProducer: You can find the producer on the/producerdirectory of the repo.  The producer is a HTTP API that publishes the request body to Kafka, and it also includes a web interface to make HTTP requests to produce messages.Consumer: You can find the consumer on the/consumerdirectory of the repo. The consumer is a worker that listens to events from Kafka and prints the messages.Managed Kafka: This is the managed Kafka instance created in Step 1.Here is how your app would look like. Notice theproduceris aweb serviceand theconsumeris aworker.Also, in order to ensure your application can connect to Kafka correctly, the following environment variables are needed.UnsetKAFKA_BROKER=${kafkaocean.HOSTNAME}:${kafkaocean.PORT}\nKAFKA_USERNAME=${kafkaocean.USERNAME}\nKAFKA_PASSWORD=${kafkaocean.PASSWORD}\nKAFKA_CA_CERT=${kafkaocean.CA_CERT}\nKAFKA_TOPIC=datastreamStep 3 - Test Your AppThere is a web UI included in the sample app to produce a message. The text you entered in the UI can be seen in the runtime logs of the consumer component.What is Next?View YouTube videoIn the interest of being iterative in our value delivery to you, we released the Managed Kafka integration with App Platform as Beta. But we are not going to stop there!We’ll be looking at ways to secure Kafka with Apps connections and improving the user experience on the environment variables setup.Currently, we recommend usingDedicated IPsto secure yourManaged Kafkainstance to only accept connections from your application.Have ideas on how to improve this integration further? Please reach out to oursupportteam.For detailed information on App Platform’s features and capabilities, visit the App Platformproduct page. To explore pricing options, or the free tier, visit the App Platformpricing page.Happy coding!\nApp Platform Team"
    },
    {
        "heading": "How SMBs and startups scale on DigitalOcean Kubernetes: Best Practices Part IV - Scalability",
        "author": "Bikram Gupta",
        "postedAt": "Posted:June 6, 2024•11 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/how-isvs-startups-scale-digitalocean-kubernetes-best-practices-scalability",
        "content": "IntroductionThis article is part of a 6-part series on DigitalOcean Kubernetes best practices targeted at SMBs and startups.InPart 1 of the series, we covered the challenges in adopting and scaling Kubernetes, and  “Developer Productivity” best practices. We explored how Kubernetes can streamline development processes, increase efficiency, and enable faster application time-to-market.InPart 2 of the series, we covered “observability” best practices. We discussed the importance of monitoring, logging, and tracing in a Kubernetes environment and how these practices contribute to maintaining a healthy and performant system.InPart 3 of the series, we covered “reliability” best practices. We discussed right-sizing your nodes and pods, defining appropriate Quality of Service (QoS) for pods, utilizing probes for health monitoring, employing suitable deployment strategies, optimizing pod scheduling, enhancing upgrade resiliency, and leveraging tags in your container images.In this current segment (Part 4), we focus on scalability. The meaning and scope of scalability can be different for a small and medium-sized business (SMB) compared to an enterprise. SMBs often have limited resources and smaller-scale deployments, which present unique challenges in ensuring the scalability of their Kubernetes clusters. We start with a broad overview of the challenges. Then, we provide a set of checklists and best practices that SMBs can follow to ensure scalability in their Kubernetes environments. Our focus is primarily on SMB-scale clusters, typically consisting of fewer than 500 nodes.Scalability Challenges“There became a point where we had revenue coming in and there was a fear that we couldn’t deliver at scale for a large event, with many concurrent users and more than a single stream per user.CTO.aicame in and saw what our current infrastructure was and highlighted the solutions such as DigitalOcean Kubernetes that could scale up and down automatically.”- Andrew Lombardi, Snipitz Chief Product OfficerScalability challenges are common within Kubernetes and occur when the cluster fails to scale or scales too slowly to meet the demand. These issues can include:Insufficient resources:Lack of available nodes or resources to accommodate the growing workload.Slow autoscaling:Autoscaling mechanisms do not react quickly enough to sudden spikes in traffic.Inefficient resource utilization:Suboptimal resource allocation or overprovisioning leads to wasted resources and increased costs.Database scalability:Scaling databases to handle increased read/write operations is difficult.To address these challenges, adhering to well-established practices in Kubernetes and cloud-native computing while considering your application’s specific requirements and characteristics is crucial.Defining Kubernetes ScaleScaling brings its own set of challenges and is highly dependent on the dimension of scale. Our definition of Kubernetes scale focuses on cluster sizes up to 500 nodes and running different types of applications at scale (e.g., data analytics, web scraping, metaverse, video streaming, etc.).We can define scale at different layers as follows. The list below is not exhaustive but contains some key points to consider when considering scalability.1. Cluster Scalability:Cluster auto-scaling:Does the cluster auto-scaler rapidly and seamlessly scale from 10 nodes to 100+ nodes when needed?Kubernetes API server performance:Can the API server handle the increased load and maintain low latency when scaling to 100+ nodes?Cluster DNS scalability:Does the cluster DNS scale effectively to meet the demands of your growing applications?etcd scalability:Can the etcd cluster scale horizontally to handle the increased data and traffic while ensuring high performance and availability?Network scalability:Does the cluster network scale seamlessly with the growing number of nodes and pods, maintaining low latency and high throughput?2. Application Scalability:Horizontal pod auto-scaling:Can you leverage the Horizontal Pod Autoscaler (HPA) to automatically scale your application pods based on metrics like CPU utilization or custom metrics?Buffer nodes:Are pre-provisioned buffer nodes available to accommodate sudden spikes in application traffic?Load balancer scalability:Does the load balancer scale to meet your application demands and distribute traffic evenly across the application pods?Database scalability:Can your database solution scale based on the application demand using techniques like sharding, replication, or distributed databases?Caching and content delivery:Do you employ caching mechanisms and Content Delivery Networks (CDNs) to reduce backend load and improve response times?Message queue scalability:If message queues are used, can they scale to handle the increased message volume, and can you horizontally scale the message queue consumers?Scalability is not just about adding more resources but also about designing your applications and infrastructure to be scalable from the ground up. By asking the right questions and proactively addressing scalability concerns, startups can build Kubernetes environments that can more effectively handle the demands of their growing business. Regular testing, monitoring, and optimization are crucial to ensure a smooth scaling experience.Scaling Best PracticesScaling is a complex and multifaceted domain encompassing various system aspects, including API, network, DNS, load balancer, and application. In this section, we will focus on the top problem areas that users commonly encounter when scaling their applications on DigitalOcean Kubernetes. We assume you have already implementedobservability measuresand followed the appropriate checklist described in the reliability section. Additionally, this series will cover disaster recovery and security topics in future blog posts.Checklist: Know your Default Account LimitsDigitalOcean accounts havedefault limits, with newer accounts having tighter restrictions than established ones. Kubernetes users should pay attention to the limits on Droplets, load balancers, firewalls, firewall rules, and Volumes.Teams can visit their respective team page to review and request adjustments to Droplet limits. Limits on other resources are not publicly exposed but can be modified by contactingDigitalOcean support.As Kubernetes customers enable autoscaling, they may need to adjust their account limits to accommodate their cluster’s expected maximum size. When considering account limits, keep the following points in mind:Droplet account limits:Ensure that your nodes can scale out sufficiently to handle the expected maximum load, taking into account the max_nodes setting of each node pool and the account’s Droplet limit.Volume limits:Be aware of the total volume limit and the hard limit of seven volumes per Droplet. Verify if your applications can scale to their maximum respective limits considering the total volumes limit and the per-Droplet volume limit. Pay close attention to the affinity and anti-affinity rules in StatefulSets that have persistent volume claim templates.Load balancer limits:Adjust load balancer limits if necessary to allow the creation of Kubernetes services of type LoadBalancer.Firewall limits:DOKS provisions two firewalls per Kubernetes cluster by default. Services of type NodePort automatically add firewall rules.Checklist: Use Horizontal Pod AutoscalerDigitalOcean supports the automatic scaling of both pods and worker nodes through the Horizontal Pod Autoscaler (HPA) and Cluster Autoscaler (CA), respectively. Here’s how these components work together to optimize your deployment:The HPA automatically adjusts the number of pods in a deployment or replication controller based on observed CPU utilization or other specified metrics. When the HPA triggers an increase in the pod count to maintain set resource utilization thresholds, it ensures that your application maintains performance without manual intervention.The CA monitors the need for additional worker nodes. If the increased pod count requires more resources than currently available, CA will provision additional worker nodes to accommodate the new pods. This ensures that the cluster adapts dynamically to the load.Benefits of using HPA and CA Together: This combination allows for a highly responsive and resilient Kubernetes environment that can scale both horizontally (more pods) and vertically (more nodes) as needed.By leveraging HPA and CA, you can ensure that your Kubernetes cluster scales efficiently in response to application demands, optimizing both resource usage and cost.Checklist: Scaling applications rapidly using a buffer nodeWhen applications need to scale rapidly, the time it takes to create a new node (approximately 5 minutes) can be too long. To address this issue, a good practice is to run low-priority buffer pods that utilize the Cluster Autoscaler to proactively create one or more buffer nodes.Here’s how it works:Deploy low-priority buffer pods:Create a deployment or replica set with low-priority pods that have minimal resource requirements. Set thepriorityClassNameof these pods to a lower value than your main application pods.Configure cluster autoscaler:Set up the Cluster Autoscaler to monitor the resource utilization of the cluster. The Cluster Autoscaler will automatically create buffer nodes based on the demand from the low-priority buffer pods.Scaling behavior:When the Horizontal Pod Autoscaler (HPA) triggers the scaling of your main application pods, Kubernetes will prioritize scheduling them on the available nodes. If there is insufficient capacity on the existing nodes, Kubernetes will evict the low-priority buffer pods to make room for the higher-priority application pods. The evicted buffer pods will be rescheduled onto the newly created buffer nodes by the Cluster Autoscaler.By employing this strategy, your applications can scale instantly without waiting for new nodes to be provisioned. The buffer nodes act as a pre-provisioned resource pool, allowing for rapid scaling when demand increases. This approach helps maintain the responsiveness and availability of your applications during sudden spikes in traffic or workload.Note that using a buffer node will incur additional charges.Checklist: Optimize Application Start-up TimeWhen rapidly scaling your cluster from a small number of nodes to a large number (e.g., from 5 to 50 nodes) to run batch jobs or handle increased workload, fetching many container images from the registry can become a bottleneck. Even though your cluster may scale quickly, the applications must fetch the required images and be ready to serve requests. This process can introduce latency and impact the overall start-up time of your applications.To optimize the application start-up time and mitigate the impact of image fetching, consider the following strategies:Use a registry in the same region:Ensure that your container image registry (or a mirror) is located in the same region as your Kubernetes cluster. This proximity helps reduce network latency and speeds up the image-fetching process.Implement a pull-through cache:Deploy a local Docker registry within your Kubernetes cluster as a pull-through cache. Configure your applications to pull images from this local registry instead of the remote registry. The local registry will cache the images pulled from the remote registry, helping to reduce the network latency for subsequent image pulls.Optimize image size:Minimize the size of your container images by removing unnecessary files and layers. Use minimal base images and leverage multi-stage builds to keep the final image size small. Smaller image sizes result in faster image downloads and shorter start-up times. For those seeking highly optimized and secure base images, consider using offerings from providers likeChainguard. These images are designed with security and minimal size in mind, which can further enhance the performance and security of your applications.Utilize image pre-pulling:Consider implementing image pre-pulling techniques to proactively fetch and cache the required images on the nodes. This can be achieved using a DaemonSet that pulls the images on each node.Checklist: DNS ScalingTo ensure efficient DNS resolution and handle increased traffic in your Kubernetes cluster, consider scaling DNS at three layers:Cluster CoreDNS:Scale CoreDNS by increasing the number of replicas in the deployment. Adjust the replicas based on CPU utilization and DNS latency.Node-Local DNS cache:Implement theNodeLocalDNSCachedaemonset to enable DNS caching on each node. Node-local caching reduces the load on CoreDNS and improves DNS performance.Cloud provider DNS forwarder:DigitalOcean manages and scales the DNS forwarder in each region. This layer handles external DNS resolution and is automatically scaled by the cloud provider.To optimize DNS performance in your cluster:Monitor CoreDNS performance and scale the number of replicas as needed.Deploy the NodeLocalDNSCache daemonset to enable node-local DNS caching.Checklist: Caching and Database ScalingDatabases work very well but also require due diligence when scaling. Help ensure your system remains efficient and responsive under heavy loads by implementing these key strategies:Caching:Implement Redis, Memcached, or some other caching mechanism to store frequently accessed data, reducing database load and improving response times.Connection pooling:Use connection pooling to manage database connections more efficiently, minimizing overhead and enhancing throughput.Database scaling:For Horizontal (sharding), partition data across multiple servers to effectively manage large datasets and high traffic volumes. You can also use read replicas to distribute read queries and balance the load in read-heavy applicationsManaged services:Consider DigitalOcean Managed Databases for automated scaling, maintenance, and security, simplifying database management.Monitoring:Continuously monitor and optimize your database and caching setup to promptly identify and address performance bottlenecks.Checklist: Network Load TestingNetwork load testing is essential for validating the performance and scalability of your application’s network infrastructure under realistic and peak load conditions. This testing helps identify potential bottlenecks and performance constraints, providing insights in advance.Implement realistic testing scenarios:Utilize tools like Apache JMeter, Locust, or Gatling to simulate real-world user behavior and traffic patterns. Ensure these scenarios cover both expected traffic and extreme load conditions to thoroughly test the network’s capability.Incorporate chaos testing:In addition to standard load testing, integrate chaos testing methods to evaluate how your network handles unexpected disruptions. Tools such as ChaosMesh, LitmusChaos, or ChaosMonkey can introduce random failures into your network components (like randomly terminating instances) to test the resilience and failover mechanisms.Checklist: Resilience to Kubernetes API Latency and FailureInteracting with the Kubernetes control plane, particularly the kube-apiserver, is a common requirement for applications that dynamically manage resources within the cluster. Given the complexity and distributed nature of the control plane, these interactions can be susceptible to various failure modes, including network latency and system errors.Implement strong  communication practices:Retries:Start by implementing retry mechanisms in your applications. This basic resilience strategy can handle intermittent failures by attempting the request multiple times.Backoffs: To prevent overwhelming the API server during high latency or failure periods, integrate exponential backoff logic in your retries. This approach gradually increases the wait time between retries, reducing the load on the server and improving the chances of recovery.Circuit Breaking:As a more advanced strategy, implement circuit breakers to stop cascading failures in an interconnected system. Circuit breakers can temporarily halt operations to a failing service until stability is restored, preventing failures from spreading across the system.Handle failure responses gracefully:Be proactive in handling HTTP error responses from the Kubernetes API. Common errors include 5xx (server-side problems) and 4xx ( client-side request issues). For example, a 503 Service Unavailable error might occur when requests to the underlying etcd database fail.Design your application to understand and react appropriately to these errors, potentially logging incidents for further investigation or triggering alternative workflows to maintain operational stability.In conclusion, scaling your applications on DigitalOcean Kubernetes requires careful consideration of various factors, including account limits, autoscaling mechanisms, application start-up time, DNS scaling, caching and database optimization, network load testing, and resilience to Kubernetes API latency and failures. By following the checklists and best practices outlined in this guide, you can more effectively scale your applications, handle increased traffic, and ensure optimal performance and reliability in your Kubernetes cluster.Next StepsAs we continue to explore the ISV journey of Kubernetes adoption, our ongoing blog series will delve deeper into the resilience, efficiency, and security of your deployments.Scalability (this blog):Explore how to manage application scaling by asking the right questions and proactively addressing scalability concerns, SMBs can build Kubernetes environments that can effectively handle the demands of their growing business.Disaster preparedness (Part 5):Discuss the importance of having a solid disaster recovery plan, including backup strategies, practices, and regular drills to ensure business continuity.Security (Part 6):Delve into securing your Kubernetes environment, covering best practices for network policies, access controls, and securing application workloads.Each of these topics is crucial for navigating the complexities of Kubernetes and enhancing your infrastructure’s resilience, scalability, and security. Stay tuned for insights to help empower your Kubernetes journey.Ready to embark on a transformative journey and get the most from Kubernetes on DigitalOcean? Sign up forDigitalOcean Kubernetes here."
    },
    {
        "heading": "How SMBs and startups scale on DigitalOcean Kubernetes: Best Practices Part III - Reliability",
        "author": "Bikram Gupta",
        "postedAt": "Posted:May 24, 2024•16 min read",
        "portal": "Digital Ocean",
        "category": "engineering",
        "domain": "https://www.digitalocean.com",
        "link": "https://www.digitalocean.com/blog/how-isvs-startups-scale-digitalocean-kubernetes-best-practices-reliability",
        "content": "IntroductionThis article is part of a 6-part series on DigitalOcean Kubernetes best practices targeted at SMBs and startups. InPart 1 of the serieswe covered the challenges in adopting and scaling on Kubernetes, as well as “Developer Productivity” best practices, and inPart 2 of the serieswe covered “observability” best practices, including the importance of monitoring, logging, and tracing in a Kubernetes environment.In this current segment (Part 3), we focus on reliability. The meaning and scope of reliability can be different for a small and medium-sized business (SMB) compared to an enterprise. SMBs often have limited resources and smaller-scale deployments, which present unique challenges in ensuring the reliability of their Kubernetes clusters. We start with a broad overview of the challenges. Then we provide a set of checklists and best practices that SMBs can follow to help ensure reliability in their Kubernetes environments. Our focus is primarily on SMB-scale clusters, typically consisting of fewer than 500 nodes.Reliability ChallengesReliability refers to an application’s ability to work as expected in different situations. When things do not work as intended, the problems manifest in various forms, for example:Functionality: The application simply does not work. Common issues include:Pod crash-looping: Pods continuously restart due to application crashes or configuration issues.Incorrect load balancer/ingress configuration: Misconfigured load balancers or ingress controllers prevent traffic from reaching the application.Certificate not renewed: Expired SSL/TLS certificates cause connection failures.Incorrect resource limits or requests: Improper resource allocation leads to pod eviction or resource starvation.Missing or misconfigured dependencies: Incomplete or incorrect configuration of application dependencies (databases, message queues, etc.) prevents the application from functioning correctly.Latency: While the application may work fine for the most part, a percentage of requests may experience slow response times. Latency issues can arise from various factors:Network congestion: Insufficient network bandwidth or network bottlenecks lead to increased latency.Compute resource limitations: Inadequate CPU or memory allocation for the application results in slower processing times.Database performance: Slow database queries or insufficient database resources cause delays in data retrieval.External dependencies: Latency in external services or APIs that the application relies on can impact overall response times.To address these challenges, it is crucial to adhere to well-established practices in Kubernetes and cloud-native computing while considering the specific requirements and characteristics of your application.Reliability Best PracticesImplementing effective reliability in a Kubernetes environment requires a structured approach and adherence to best practices. Here’s a checklist of key recommendations.Checklist: Right-size your nodesUnderstand Your Application RequirementsUnderstanding the specific demands of your application is crucial. This involves determining whether your application primarily requires high computational power (compute-bound), significant memory (memory-bound), or intensive data transfer (I/O-bound). Once the type of application is identified, accurately assess the needed CPU, RAM, and storage to ensure that the nodes are neither over-provisioned nor under-provisioned.It is common to encounter challenges such as underutilized compute resources, which can happen when a compute-heavy application does not fully utilize the CPU capabilities while maxing out memory. Another challenge is the inability to schedule new pods on nodes that are 60% utilized but lack sufficient free resources to meet new pod requirements. Additionally, imbalances in resource utilization across nodes can lead to inefficiencies, where some nodes may be CPU-heavy and under-utilize memory and vice versa.Choose the Right Node Size and TypeChoosing the correct node size involves selecting nodes that fit the resource profile of your application closely. Oversizing can lead to wasted resources and unnecessary costs, whereas undersizing might hinder performance. For better scalability and flexibility, smaller nodes can be beneficial, allowing more granular scaling and resource allocation that can adapt more fluidly to changing demands. However, in certain scenarios, such as running large, monolithic applications or workloads with high memory requirements, choosing larger nodes can be advantageous. Larger nodes can provide increased performance, reduce fragmentation and the overhead of inter-node communication. The choice of node type should be tailored to the application’s requirements: shared nodes are generally adequate for development/staging resources; dedicated nodes suit applications that are resource-intensive or require low latency; compute-optimized nodes are ideal for CPU-intensive tasks; and memory-optimized nodes should be used for applications that consume large amounts of memory.Here are some guidelinesfor right sizing nodes for your application.Use Node Pools and Leverage Scheduling FeaturesNode pools allow for the grouping of nodes that serve similar types of applications or workloads, facilitating better resource management and allocation efficiency. By grouping applications that share similar resource needs into the same node pool, you can enhance both performance and resource utilization, making it easier to manage and scale applications dynamically.Kubernetes scheduling features such as pod affinity and anti-affinity can significantly improve the efficiency of resource utilization. Pod affinity rules help in placing related pods close to each other to reduce latency, while anti-affinity rules prevent resource contention by avoiding placing too many similar resource-demanding pods on the same node.Checklist: Right-size your podsIt is important when deploying resources inside your Kubernetes cluster that you indicate how much CPU and memory RAM you want to allocate to your deployed resources. By describing those constraints, you are helping the kube-scheduler control plane component make better bin packing decisions when assigning your pods to your worker nodes. Once your pods are assigned on worker nodes, the kubelet component will then ensure that the request and limits specified on the pod’s containers are enforced.For both the CPU and RAM requests, the container runtime will guarantee the specified resource values. Thus, you need to ensure that your biggest worker node pool contains worker nodes that can accommodate for the specified CPU or RAM request, or else you could end up in a situation where your pods can’t be scheduled on any worker nodes and could result in downtime for your application. Also, since the request value will guarantee the reservation of those resources, it is important that you are not over-requesting resources and pay for extra resources that you don’t need or could be used for other workloads.Next, is the CPU and Memory (RAM) limits. Kubernetes will enforce those limits and prevent deployed resources from using more than what they are given. This is important to understand but CPU and RAM limits behave differently. When a CPU limit is reached, your pod containers will be throttled. This throttling could lead to degraded performance of your applications especially if they are running against those CPU limits values for a prolonged period of time. This could be a good opportunity to revisit your pod container limit value when an issue like that occurs and give enough CPU to your workloads. That said, even if that throttling could lead to degraded performance, the containers won’t be terminated or evicted by the container runtime since the CPU is a compressible resource. Your application can remain accessible but the performance will likely be altered. Be careful when setting limits. By omitting CPU limits, processes will be able to use any additional CPU capacity available on the node. When CPU limits are set, applications may be CPU throttled even when the node has otherwise idle CPU capacity. As a general rule of thumb, we recommend that your application omits the CPU limits entirely unless you have strict needs that implies setting them.On the other hand, when the pod memory consumption limit is reached, if the container allocates more memory than it is allowed to, the container will become a candidate for termination. You will see a status of OOM (Out Of Memory) on the pod status when that termination occurs. This behavior is important since terminated pods could cause downtime for your applications. Even if your pods are controlled by aReplicaSetor part of aDeployment, until the new pods are scheduled and back in a running state, your customers could be affected by OOM terminated events.Checklist: Assign quality of service to podsKubernetes offers three Quality of Service (QoS) classes. Those QoS classes are used by Kubernetes when there are no more available resources on a given node and you need to make a decision regarding which pod needs to be evicted. The QoS class assigned to the pod will determine the eviction priority. There are three choices of QoS classes that are assigned to a pod on creation:BestEffort,BurstableandGuaranteed. When an eviction decision needs to be made, the pods with aBestEffortQoS will be terminated first, followed byBurstableand lastlyGuaranteed. Here are the criteria that dictate which category a pod will fall into:BurstableThe Pod does not meet the criteria for QoS classGuaranteed.At least one Container in the Pod has a memory or CPU request or limit.GuaranteedEvery Container in the Pod must have a memory limit and a memory request.For every Container in the Pod, the memory limit must equal the memory request.Every Container in the Pod must have a CPU limit and a CPU request.For every Container in the Pod, the CPU limit must equal the CPU request.BestEffortDoes not meet the criteria ofBurstableorGuaranteedThat said, it is important to consider the right CPU request and limit to fall into the right QoS class based on your application needs. For example, for critical applications, you will want them to have a QoS of Guaranteed so they are prioritized during scheduling by the kube-scheduler and evicted last if nodes are running out of resources.To optimize CPU and memory allocations in Kubernetes, you have several options:Use the Vertical Pod Autoscaler (VPA) with the Metrics Server. The Metrics Server collects real-time resource usage data, which VPA analyzes to provide recommendations on CPU and memory limits based on historical usage patterns.LeverageFairwinds Goldilocks, which is built on top of VPA and offers a user-friendly interface for resource recommendations.Use RobustaKubernetes Resource Recommender (KRR), a lightweight approach that utilizes Prometheus and cAdvisor to generate resource recommendations. It does not require any agent, and you can simply run a CLI tool on your local machine for immediate results.Each of these tools offers a unique approach to resource optimization, allowing you to choose the one that best fits your cluster’s needs and existing infrastructure.Checklist: Use Probes for Health ChecksKubernetes container probes are a key concept for ensuring the reliability of our applications. In this next section, we’ll discuss in more detail the different types of probes, when to use them, how to use them, and their key role they have in the reliability of our system. First, the definition of a probe is to search into and explore very thoroughly.  You can either use the HTTP protocol and make a request against our application, execute a shell command, try to open a TCP socket to our container on a specified port, or perform a gRPC health check. Any of those mechanisms can be used to interact with our application and determine its health. Let’s review the three different use cases for Kubernetes probes.The first probe we’ll cover is theStartup probe, this probe is useful when you have applications that have a slow startup. This probe is used to know when the application has started. This will also block the liveness probe and the readiness probe from running until the startup probe succeeded. You can define the startup probe in the container spec with looser values and usually higher failure thresholds and delays than the readiness probe or liveness probe since the latter need to detect problems with a quicker response time.The second probe is theReadiness probe. This probe indicates if the container is ready to accept and serve traffic. Usually, applications will have some tasks to do when they initialize or during their runtime. For example, they might pull secrets from an external vault, load their cache, interact with different services and do other tasks which should prevent them from receiving traffic. That said, you don’t want to send traffic to those pods/containers until they are fully ready to accept and serve the traffic. If those pods are targeted by a Kubernetes service and they are reporting that they are not in aReadystate, they won’t receive traffic. Omitting the readiness probe on a pod container definition backing a Kubernetes service, by default, will consider the container asReadyand will immediately be considered for receiving traffic. This is a scenario to be avoided since it could result in errors in your system for a brief period of time until the container finishes its bootstrapping.The last probe we’ll cover is theLiveness probe. Once your application is running smoothly, we still want to ensure that it stays that way and if any potential issues are detected that are unrecoverable and would require a pod restart, you can use the liveness probe for that. This probe will periodically check our application health and if the check fails, it will terminate the pod. A typical use case for liveness probes is for a temporary mutation of a detectable condition that can be handled via a restart (e.g. memory corruption, a memory leak, deadlocks) until the root cause of the problem can be found. You want to ensure that the readiness probe fails first, turning the pod in aNotReadystate which causes the traffic to stop going to that pod but keeping the possibility of the pod returning back in aReadystate. Then, if the failure is maintained for a prolonged period of time, it seems like you are in an unrecoverable state in which it would make sense for the liveness probe to fail and trigger a pod restart.Finally, something that you should be aware of is that probes should not have external dependencies. The last thing you want to introduce in our cluster are cascading events that could cause many applications to have their probes fail which could lead to unresponsiveness or a lot of pod restarts. Thus, we recommend that probes logic does not interact with external components, APIs, third party services etc. where one failure could cause many applications to fail.Checklist: Deployment strategiesIn this section, we’ll go over two deployment rollout strategies that you can introduce in your continuous delivery pipeline to make your system more reliable when introducing changes to your production system. We’ll also assume that you have correctly instrumented your applications with metrics and you are able to track the fourgolden signals(Latency, Traffic, Errors, Saturation).The first deployment strategy we’ll cover is thecanarydeployment. A canary deployment rolls out a small set of replicas with the new release and monitors the behavior with production traffic. If the new rollout is behaving correctly, you can increase the number of replicas using the new version. If it is the opposite, you can quickly revert those replicas, and gather some useful information on the failure domains from the logs and metrics. If an issue was detected on the canary deployment, you can be assured that it only affected a small percentage of the production traffic. In the Kubernetes context, consider the following setup.As you can see, the green canary rollout percentage can easily be changed by increasing or decreasing the amount of replicas of thecanarydeployment. Depending on your application’s needs and the 4 golden signals that you gathered by this deployment initiative, you can determine the actions to take by reverting or pursuing the completion of the new version.Another rollout strategy that can be easily set up with Kubernetes is the Blue-Green rollout strategy. With this strategy, you’ll introduce a new version by having two separate deployments. One deployment with the new version (Green) while the other contains the current version (Blue).As you can see just by changing the Kubernetes service selector field to match the newv2version labels, you can redirect the entire traffic to the green deploymentv2. Depending on our 4 golden signals metrics you gathered, if our tests are successful, you can drop the blue deployment (old version) and continue with the green deployment (new version). In the event the tests are unsuccessful, can easily revert back to the blue deployment, tear down the green deployment, and review what went wrong from the gathered metrics and logs.Checklist: Use Pod scheduling (Affinity rules & Pod topology)In DigitalOcean Kubernetes, your cluster consists of one or multiple worker nodes (Droplets) running your applications. This means that if your application pods are not spread out to one or more worker nodes, they could be impacted by a failure on the worker node itself. Since the  DOKS platform supports only the last 3 minor versions, eventually your cluster will have to undergo a cluster upgrade to the last supported DOKS version. This will trigger a rollout of your worker nodes and could cause some reliability issues if you don’t spread your applications throughout multiple nodes. This spread issue could also surface when recycling manually worker nodes or if you are using node autoscaling. This issue can be solved by using pod topology spread constraints. This Kubernetes feature is really powerful since you can control how pods are spread across your cluster’s nodes. If we take an example of a DOKS cluster with 3 worker nodes running a deploymentApp1of 4 replicas.You want to avoid the following scenario:In the above example, all theApp1deployment pods are scheduled on the same worker node. If theWorker node 1is removed for any of the reasons mentioned above, the application will experience some downtime. Now let’s introduce thetopologySpreadConstraintsto our deployment resource.With this new configuration you can now survive a worker node termination without any downtime for the applicationApp1. The above example is just touching the surface of what topology spread constraints allows you to do but if you want more information about the subject and more advanced configuration, you can access theKubernetes documentation on more advanced topics.Checklist: Plan for Smooth UpgradesWhen planning for a smooth upgrade of DigitalOcean Kubernetes (DOKS), it’s crucial to test your application thoroughly to ensure it remains compatible with rapidly evolving Kubernetes APIs. Understanding the specifics of how DOKS manages upgrades is also essential. DigitalOcean performs surge upgrades by default, which involves creating up to ten additional worker nodes at a time in a pool and then recycling the existing nodes. While this method aims to minimize downtime, it can still disrupt workloads if not managed carefully.To mitigate potential disruptions during cluster upgrades, which are a common cause for node shutdowns, it’s important to implement graceful shutdown procedures for your pods. For stateful workloads, which require more time to drain from a node, DOKS provides a grace period of up to 30 minutes for node drainage. Additionally, employing a pod disruption budget is recommended to protect stateful workloads during these upgrades. This budget helps ensure that a minimum number of pods remain running during the node replacement process, preserving the integrity and availability of your services.Checklist: Use unique tags in your images to avoid different versions in containersWhen creating a new pod, the default image pull policy in the container specification ensures that the image is pulled only if it doesn’t already exist on the node where the pod is scheduled to run. This can lead to inconsistencies if you reuse the same tag name, such as “latest,” for different builds of your application.Consider a scenario where you build your code into an image, push it to your container registry using the “latest” tag, and create a deployment using that tag in your DOKS cluster. The first time you deploy your pods, all nodes will pull the image from the registry since they don’t have it cached locally. However, if you update your code, build a new container image, and push it with the same “latest” tag, triggering a new rollout, the pods running on nodes that already had the previous “latest” image will continue running the old code without you noticing. However, any pod on a new node will pull the “latest” image, resulting in an inconsistent application version.The issue becomes more insidious when you push an image without explicitly specifying a tag. In such cases, the image is implicitly tagged as “latest.” Moreover, even without performing a new rollout, any pod replacement that occurs after the “latest” image is updated will run the most recent version of the image tagged as “latest.”To prevent these inconsistencies and help ensure that your pods always run the intended version of your application, it is highly recommended to use semantic versioning tags or the commit SHA to uniquely identify each build. This practice helps differentiate between different binaries and configurations, preventing collisions and ensuring that the correct version of your application is deployed consistently across all pods in your cluster.Summary - Reliability Best PracticesIn summary, ensuring the reliability of your applications on DigitalOcean Kubernetes involves implementing best practices across various aspects of your deployment as discussed above. This includes right-sizing your nodes and pods, defining appropriate Quality of Service (QoS) for pods, utilizing probes for health monitoring, employing suitable deployment strategies, optimizing pod scheduling, enhancing upgrade resiliency, and leveraging tags in your container images. By following the checklists and recommendations provided in this guide, you can build reliable and resilient applications that can withstand failures, recover gracefully, and maintain optimal performance in your Kubernetes cluster.Some vendors specialize in enhancing Kubernetes reliability by proactively identifying and resolving issues, as well as refining troubleshooting processes. Some notable ones areKomodor,Robusta, andFairwinds.Next StepsAs we continue to explore the ISV journey of Kubernetes adoption, our ongoing blog series will delve deeper into the resilience, efficiency, and security of your deployments.Scalability (Part 4): Explore how to manage zero-downtime deployments, readiness/liveness probes, application scaling, DNS, and CNI to maintain optimal performance under varying loads.Disaster preparedness (Part 5): Discuss the importance of having a solid disaster recovery plan, including backup strategies, practices and regular drills to help ensure business continuity.Security (Part 6): Delve into securing your Kubernetes environment, covering best practices for network policies, access controls, and securing application workloads.Each of these topics is crucial for navigating the complexities of Kubernetes, enhancing your infrastructure’s resilience, scalability, and security. Stay tuned for insights to help empower your Kubernetes journey.Ready to embark on a transformative journey and get the most from Kubernetes on DigitalOcean? Sign up forDigitalOcean Kubernetes here."
    }
]